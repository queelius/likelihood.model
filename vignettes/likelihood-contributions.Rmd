---
title: "Likelihood contributions model"
output:
    rmarkdown::html_vignette:
        toc: true
        toc_depth: 2
vignette: >
  %\VignetteIndexEntry{Likelihood contributions model}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

## Installation

You can install the development version of `likelihood.model` from
[GitHub](https://github.com/queelius/likelihood.model) with:

```{r install-likelihood-model, eval = FALSE}
if (!require(devtools)) {
    install.packages("devtools")
}
devtools::install_github("queelius/likelihood.model")
```


```{r load-packages}
library(algebraic.mle)
library(likelihood.model)
```


## Exponential series system

Consider a series system with $m$ components, where the $p$th component in the $i$-th system has lifetime $T_{i p}$ with rate $\lambda_p$, and $T_{i j}$
for all $i,j$ are independent. The system lifetime is the minimum of the component lifetimes, i.e., $T_i = \min\{T_{i 1}, \ldots, T_{i m}\}$.

Let's draw a sample for this model for $m=3$ and $\lambda = (1, 1.25, 1.5)$
for a sample size of $n=75$.
```{r data-gen-exp-series}
n <- 70
rates <- c(1.1, 1.25, 1.5)
set.seed(123)

df <- data.frame(t1 = rexp(n, rates[1]),
                 t2 = rexp(n, rates[2]),
                 t3 = rexp(n, rates[3]))
df$t <- apply(df, 1, min)

# map each observation to the corresponding index (component) that was minimum
df$k <- apply(df, 1, which.min)

head(df)
```

Column $t_p$ is the failure time of the $p$-th component, column $t$ is the 
column for the series system lifetime, and column $k$ is the component that 
causes the system failure.

Of course, normally we don't know the component lifetimes, but only the
series system lifetime (or a censored version of it). However, sometimes,
we may know more or less. We will consider three cases:

1. We know the component lifetimes exactly. (Columns $t_1, \ldots, t_m$ are 
observed.)
2. We know the system lifetime and the component cause of failure. (Columns $t$
and $k$ are observed.)
3. We know the system lifetime, and some subset of the components that contains
the component that caused the system failure. (New columns will be introduced
to indicate which components are observed.)

### Case 1: Complete knowledge of component lifetimes
Suppose we have a series system, but we have complete knowledge of the component lifetimes. The component lifetimes are exponentially distributed with rate $\lambda_k$ for component $k$.

The system lifetime is the minimum of the component lifetimes, but we actually know the component lifetimes somehow, so we just estimate each $\lambda_k$ separately ($m$ independent MLE problems), which we know to have the MLE
$\hat{\lambda}_p = 1/{\bar{t}_p}$ where $\bar{t}_p$ is the sample mean of the component lifetimes for component $p$.

```{r case-1-mle}
(lambda.hat <- c(1/mean(df$t1), 1/mean(df$t2), 1/mean(df$t3)))
```


#### Likelihood model

Let's derive the full likelihood model for this problem anyway.
The likelihood function is given by
so we use those instead. The likelihood function is given by
$L(\lambda_1, \dots, \lambda_p) = \prod_{i=1}^n \prod_{j=1}^m \lambda_j \exp(-\lambda_j t_{i j})$, whose log-likelihood function is given by $\ell
(\lambda_1, \dots, \lambda_p) = \sum_{i=1}^n \sum_{j=1}^m \log \lambda_p - 
\lambda_p t_{i j}$, whose score function is given by $s(\lambda_1, \dots, 
\lambda_p) = (\partial \ell / \partial \lambda_1 \cdots
    \partial \ell / \partial \lambda_m)^T$ where $\partial \ell / \partial 
    \lambda_p = n/\lambda_p - \sum_{i=1}^n t_{i p}$,
and whose Hessian matrix is a diagonal matrix whose $(p,p)$-th element is given 
by $\partial^2 \ell / \partial \lambda_p^2 = -\frac{n}{\lambda_p^2}$.

To solve the MLE, we just solve the score equations $s(\lambda_1, \dots,
\lambda_p) = 0$, which has the unique solution
$\hat{\lambda}_p = 1/{\bar{t}_p}$.

So, the sampling distribution of $\hat\lambda_p$ is given by
$$
\hat\lambda_p \sim \text{Gamma}(n, 1  / \lambda_p),
$$
and asymptotically, by the CLT, it is given by
$$
\hat\lambda_p \sim \text{Normal}(\lambda_p, \lambda_p^2 / n),
$$
which we may in either case replace $\lambda_p$ with $\hat\lambda_p$ to get
respectively the approximate sampling and approximate asymptotic sampling distribution of $\hat\lambda_p$.

```{r case-1-likelihood-model}
model.exp.complete <- likelihood_contr_model$new(
    obs_type = function(row) {
        "observed"
    },
    logliks = list(
        observed = function(row, par) {
            dexp(row$t1, par[1], log = TRUE) +
            dexp(row$t2, par[2], log = TRUE) +
            dexp(row$t3, par[3], log = TRUE)
        }
    ),
    scores = list(
        observed = function(row, par) {
            c(1/par[1] - row$t1,
              1/par[2] - row$t2,
              1/par[3] - row$t3)
        }
    ),
    hess_logliks = list(
        observed = function(row, par) {
            -matrix(c(1/par[1]^2, 0         , 0,
                      0         , 1/par[2]^2, 0,
                      0         , 0         , 1/par[3]^2),
                nrow=length(par))
        }
    )
)
ll.exp.complete <- loglik(model.exp.complete)
s.exp.complete <- score(model.exp.complete)
H.exp.complete <- hess_loglik(model.exp.complete)
```

The MLE is just:
```{r case-1-mle-likelihood}
res.exp.complete <- optim(rates, fn = ll.exp.complete, gr = s.exp.complete,
    df = df, hessian = TRUE, control = list(fnscale = -1))
print(res.exp.complete$par)
print(lambda.hat)
ll.exp.complete(df, res.exp.complete$par)
print(res.exp.complete$value)
print(res.exp.complete$hessian)
print(H.exp.complete(df, res.exp.complete$par))
```

We see that the two MLEs, the analytical one and the numerical one, are the same.

The sampling distribution of the MLE is given by:
```{r case-1-mle-sampling}
mle.samp <- mle_numerical(res.exp.complete)
summary(mle.samp)
```


### Case 2: Known component cause, but masked component lifetimes

In this case, we assume we can only observe the system lifetime and the component that caused the system failure.

We might be tempted to think that we can just consider each observation, where
we know the system lifetime and the componente cause, and then just estimate
the component lifetime for that component based on its failure time. We have
less information about the parameters, because each observation only gives us
information about one component, but that's okay.

However, this is incorrect, because when we condition on the component that
caused the system failure, we are not observing a random sample of that
component's lifetime, but a conditional sample. For instance, if $k = 1$,
then we are observing
$T_{i 1} | T_{i 1} < T_{i 2} \text{ and } T_{i 1} < T_{i 3}$.

What is this distribution? We can derive it as:

$$
    f_{T_i|K_i}(t_i | k_i) = f_{T_i,K_i}(t_i,k_i) / f_{K_i}(k_i) =
        f_{K_i|T_i}(k_i|t_i) f_{T_i}(t_i) / f_{K_i}(k_i).
$$
By the memoryless property of the exponential distribution, we have
$$
    f_{K_i|T_i}(k_i|t_i) = f_{K_i}(k_i),
$$
since the failure rates are constant (and thus independent of the time), and
thus the probability that a componet is the cause of a system failure is
independent of the time of the system failure.

That means that 
$$
    f_{T_i|K_i}(t_i | k_i) = f_{T_i}(t_i),
$$
which is the density of the series system. Since the minimum of exponentially
distributed random variables is exponentially distributed with a falure rate
equal to the sum of the failure rates of the components, this estimator
is an estimator of the sum of the failure rates (or the failure rate of the
series system). Let's do this computation:

```{r series-likelihood, cache=TRUE}
model.exp.series <- likelihood_contr_model$new(
    obs_type = function(row) {
        "observed"
    },
    logliks = list(
        observed = function(row, rate) {
            dexp(row$t, rate[row$k], log = TRUE)
        }
    )
)
ll.exp.series <- loglik(model.exp.series)
res.exp.series <- optim(rates, fn = ll.exp.series, df = df, hessian = TRUE,
    control = list(fnscale = -1))

print(res.exp.series$par)
```

Each of these are more or less reasonable estimates of the true failure rate
of the system, which is $(1.1 + 1.25 + 1.5) = 3.85$.

We can combine each of these estimates into a weighted mean to get
a better estimate (weighted by $n_j/n$ where $n_j$ is the number of
observations of type $j$ and $n$ is the total number of observations),
but this is not a likelihood-based estimator.

#### Joint distribution of $T_i$ and $K_i$

Instead of maximizing the conditional likelihood based on $T_i | K_i$,
we should be maximizing the joint likelihood of $T_i$ and $K_i$.

We can also use method dispatching, so that when we create a
likelihood contribution model, we only need to specify the
observation types, and it can dispatch to log-likelihood
function based on the observation type.

```{r}
score.observed <- function(row, rate, ...) {
    v <- rep(-row$t, length(rate))
    v[row$k] <- v[row$k] + 1 / rate[row$k]
    v
}

hess_loglik.observed <- function(row, rate, ...) {
    p <- length(rate)
    H <- matrix(0, p, p)
    H[row$k, row$k] <- -1 / rate[row$k]^2
    H
}

loglik.observed <- function(row, rate, ...) {
    log(rate[row$k]) - sum(rate) * row$t
}
```

We see that we're just defining functions normally, which is nice
for re-use, and it is common to have a large set of these kinds of
functions. Now, to make use of them, you only need to define the
`ob_type` function to correctly dispatch.

```{r loglik-exp-series-known}
model.exp.known <- likelihood_contr_model$new(
    obs_type = function(row) "observed"
)
```

And now we can find the MLE as before:
```{r mle-exp-series-known}
rate.comp_cause_known.mle <- mle_numerical(
    optim(
        rates,
        fn = loglik(model.exp.known),
        gr = score(model.exp.known),
        df = df,
    hessian = TRUE, control = list(fnscale = -1)))

summary(rate.comp_cause_known.mle)
```

### Case 3: Masked component cause

Finally, we consider the case where we have *masked* component cause
of failures. In this example, we consider a particular model in which
the component that caused the system failure is in an observable
candidate set that may include other components too. For instance,
if we have a system with 3 components, and we observe that the system
failed at time $t$, then we may know that the component that caused the
system failure is in the set $\{1, 3\}$, but we don't know which.

#### Assumptions about the masked component cause model
In every model we consider, we assume that the candidate set $\mathcal{C}_i$
is only a function of the component lifetimes $T_{i 1},\ldots,T_{i m}$, $\theta$,
and the right-censoring time $\tau_i$. That is, the candidate set $\mathcal{C}_i$
is independent of any other factors (or held constant for the duration of the
experiment), like ambient temperature, and these factors also have a neglible
effect on the series system lifetime and thus we can ignore them.

In the candidate set model, we make the following assumptions about
how candidate sets are generated:

  - $C_1$: The index of the failed component is in the candidate set,
  i.e., $\Pr\{K_i \in \mathcal{C}_i\} = 1$, where
  $K_i = \arg\min_j \{ T_{i j} : j = 1,\ldots,m\}$.

  - $C_2$: The probability of $C_i$ given $K_i$ and $T_i$ is equally
  probable when the failed component varies over the components in the candidate
  set, i.e., $\Pr\{\mathcal{C}_i=c_i|K_i=j,T_i=t_i,\theta\} = \Pr\{C_i=c_i|K_i=j',T_i=t_i\}$ for
  any $j,j' \in c_i$.
    
  - $C_3$: The masking probabilities are conditionally independent of $\theta$
  given $K_i$ and $T_i$, i.e., $\Pr\{\mathcal{C}_i=c_i|K_i=j,T_i=t_i\}$ is not a
  function of $\theta$.

Using these simplifying assumptions, we can arrive at a reduced likelihood
function that only depends on $\theta$ and the observed data and as long as our
candidate set satisfies conditions $C_1$, $C_2$, and $C_3$, our reduced
likelihood function obtains the same MLEs as the full likelihood function.

We see that
$$
    \Pr\{\mathcal{C}_i=c_i,|K_i=j,T_i=t_i\} = g(c_i,t_i),
$$
since the probability cannot depend on $j$ by condition $C_2$ and cannot depend
on $\theta$ by condition $C_3$. Thus, we can write the likelihood function as
$$
    L(\theta) = \prod_{i=1}^n f_{T_i}(t_i|\theta) g(c_i,t_i).
$$

We show that $g(c_i,t_i)$ is proportional to
$$
    g(c_i,t_i) \propto \sum_{j \in c_i} f_j(t_i|\theta_j) \prod_{l=j,l \neq j}^m R_l(t_i|\theta_l),
$$
and thus 

Note, however, that different ways in which the conditions are met will yield
MLEs with different sampling distributions, e.g., more or less efficient estimators.

```{r}
loglik.exp_exact_masked_component <- function(row, rates) {
    X <- select(row, starts_with("x"))
    -sum(rates) * row$t + log(sum(rates[X]))
}

score.exp_exact_masked_component <- function(row, rates) {
    X <- select(row, starts_with("x"))
    X / sum(rates[X]) - row$t
}
```


### Right-censoring
Finally, we can add right-censoring to the model, so that now we
have not only masked data in the form of candidate sets but also
right-censored data. We assume that the right-censoring time is
independent of the component lifetimes and the candidate set.

```{r}
loglik.exp_right_censored <- function(row, rates) {
    -sum(rates) * row$t
}
score.exp_right_censored <- function(row, rates) {
    rep(-row$t, length(rates))
}
```